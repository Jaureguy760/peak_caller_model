{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eced0e5b-57b6-42b8-860c-52dc64e3a45e",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d4582f7-d4d2-4e69-a06e-c9d965e77da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51042794-7f48-4ec6-8e93-4806599d5292",
   "metadata": {},
   "source": [
    "### Cuda debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d8dd9d-bc79-414f-a1fd-a8d53927bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f6ed1-29a4-4f25-bdbf-e604126e4978",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67fa35f-bd36-4312-bd34-b1ce96cf6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        \n",
    "    # def __getitem__(self, index):\n",
    "    #     return self.x[index], self.y[index]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53341565-bcae-4620-ab4f-a100a5ba8f44",
   "metadata": {},
   "source": [
    "# Parse fasta file and create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eba8ba0-e3ea-4ef1-8a55-c5cb21dcb404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: chr1:925238-926238\n",
      "Name: chr1:925238-926238\n",
      "Description: chr1:925238-926238\n",
      "Number of features: 0\n",
      "Seq('acagcagtatcctacctaagaagacttttgcccaaggtctttccaaacccaaga...cca')\n",
      "7291\n"
     ]
    }
   ],
   "source": [
    "# Load Fasta with biopython parser\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "# Create list for pos training data\n",
    "my_seqlist = []\n",
    "for seq_record in SeqIO.parse(open('/home/jovyan/data1/jjaureguy/nn_fasta/GSE168881/SRR13961069.fa.out'),'fasta'):\n",
    "    my_seqlist.append(seq_record)\n",
    "my_seqlist[0].seq\n",
    "\n",
    "print(my_seqlist[0])\n",
    "print(len(my_seqlist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed0cc74-e05a-4fa1-b8cb-a976b376bd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'A': 0, 'C': 1, 'T': 2, 'G': 3}, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bases = 'ACTG'\n",
    "base_map = dict(zip(bases, range(len(bases))))\n",
    "n_classes = len(base_map)\n",
    "base_map, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1a2b0e-d5f9-45c9-9c00-98c5cd83e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed98d8e9-3d5e-4ccd-9568-e1d5e5a45aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeafe5ca-e6fb-43d2-bb65-8a11632a39ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14566, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_seqlist = []\n",
    "neg_seqlist = []\n",
    "\n",
    "skipped = 0\n",
    "for i in my_seqlist:\n",
    "    fasta = i.seq\n",
    "    as_text = fasta.upper()\n",
    "    if 'N' in as_text:\n",
    "        skipped += 1\n",
    "    else:\n",
    "        seq = np.vectorize(base_map.get)(np.array(list(as_text)))\n",
    "        pos_seqlist.append(seq)\n",
    "        neg_seqlist.append(np.random.choice(seq, size=len(seq), replace=False))\n",
    "    \n",
    "seqlist = np.stack(pos_seqlist + neg_seqlist)\n",
    "#seqlist = list(filter(None, seqlist))  # This removes empty sequences.\n",
    "n_samples = len(seqlist)\n",
    "n_samples, skipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113555e6-7f19-482b-8719-b72c534726b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14566, 1000, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = one_hot(seqlist, n_classes).reshape(n_samples, -1, n_classes)\n",
    "training_data.shape\n",
    "#training_data.ndim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c22b4c-9e7b-48ae-842b-09db854eccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_log = training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b0588-d450-4eeb-a242-b032a4892152",
   "metadata": {},
   "source": [
    "# Create labels(response variable Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "229c1e98-33a4-45c3-ae72-18b3f0b6e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14566\n",
      "[1, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_var_pos = [1]*(n_samples//2)\n",
    "response_var_neg = [0]*(n_samples//2)\n",
    "\n",
    "total_response_var = response_var_pos + response_var_neg\n",
    "print(len(total_response_var))\n",
    "print(total_response_var[7282:7285])\n",
    "total_response_var = np.array(total_response_var)\n",
    "total_response_var.shape\n",
    "total_response_var.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806e385f-37d2-40ae-9384-8ec8c8a099aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "types = len(total_response_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a8872-8c72-40bd-ac38-d53ca3f10c5b",
   "metadata": {},
   "source": [
    "# Moving the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "214e63d5-a583-4210-95d6-93b6c62a13f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d60639-5404-4ddd-a81d-c3881c220f3c",
   "metadata": {},
   "source": [
    "# Splitting data set(training and test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64327697-ff1e-4918-a483-49e1fe3c698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11652, 1000, 4)\n",
      "(2914, 1000, 4)\n",
      "(11652,)\n",
      "(2914,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#print(x_log.shape)\n",
    "#print(total_response_var.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_log, total_response_var, train_size=0.8, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c92ca-2b35-4e8b-8619-0bdb584759ce",
   "metadata": {},
   "source": [
    "# Split training set into validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d03dd0-e147-4880-8137-c6b816b747ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712b45b6-b7d2-48d6-b7dd-52d6ad272b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10486, 1000, 4)\n",
      "(1166, 1000, 4)\n",
      "(10486,)\n",
      "(1166,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079dfce-f021-4d55-9eac-a3f274fb6b6b",
   "metadata": {},
   "source": [
    "# Converting dataset from np array to tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859f394-7b64-477d-a7c4-e7d3ede20bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32)).to(device)\n",
    "X_train.requires_grad = True\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32)).to(device)\n",
    "X_test.requires_grad = True\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).to(device)\n",
    "y_train.requires_grad = True\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).to(device)\n",
    "y_test.requires_grad = True\n",
    "X_valid = torch.from_numpy(X_valid.astype(np.float32)).to(device)\n",
    "X_valid.requires_grad = True\n",
    "y_valid = torch.from_numpy(y_valid.astype(np.float32)).to(device)\n",
    "y_valid.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96883db5-9274-4042-b1d9-8aeb6394fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.type()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c215e-16fe-416a-850f-54b35d480a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81193f-05e3-49ba-8cef-6219752a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1806a3-6d62-441b-bdac-7949f7746f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Apply DataSet function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50f42c33-08de-4c95-8d30-738695f07d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "y_valid = y_valid.reshape(y_valid.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d73f6536-b363-479e-91a1-aaf5b1625677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2914, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 72%-20%-8% split\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bb1e4-ba7d-407c-ae23-bd670d223757",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_valid, y_valid)\n",
    "test_dataset = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6c54b-ba9d-4e21-97e1-279a20aad6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c7acf-4d40-4db8-be90-621f50738590",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd23064-e9b8-4eb9-85de-5148068a422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset.y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d0f80-589c-468b-afe8-2c511f2ea43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset.y[0])\n",
    "#print(train_dataset[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe29cb-ef63-4d62-9951-145cecb83533",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.x.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc693b-3c27-4163-95b6-22335b87bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768658b-59e5-4b51-9c8a-f0a680764cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ab057-a770-472d-a6ef-7b9136b9732e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a29aa-5d9d-4351-9c4b-a09c6d3fab9e",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d9796-91dc-45f5-96d1-1428bc0fc99b",
   "metadata": {},
   "source": [
    "### Log regression var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb8a76-4221-4740-be5c-7b9ebd6a91bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data.reshape(n_samples, -1).shape\n",
    "x_log = training_data.reshape(n_samples, -1)\n",
    "x_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e51e13-8b7b-49f4-b1c2-86b5dc5fe8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(x_log.shape)\n",
    "print(total_response_var.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_log, total_response_var, test_size=0.1, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffb4a0-f5e3-42b0-b4e4-8a54dbf50728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce7109-fe60-45dd-a824-64f6a53a4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "epochs = 10000\n",
    "input_dim = 4000  # Two inputs x1 and x2 \n",
    "output_dim = 1 # Single binary output \n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(input_dim,output_dim)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da78abc-aa23-4fe0-9b04-efef76a06e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train,dtype=torch.float32, requires_grad=True)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32, requires_grad=True)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b96c5-4704-4321-966c-ea1cd59e448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7b55e-a437-49e0-86a8-1eddd5bee39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train,requires_grad=True)\n",
    "X_test = torch.tensor(X_test,requires_grad=True)\n",
    "y_train = torch.tensor(y_train,requires_grad=True)\n",
    "y_test = torch.tensor(y_test,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a6828-a5ca-47d7-ad2e-4866dca11f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5f10d-146a-4941-9198-d14e7d09fe62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ea1db-c5df-4d7c-b4ff-5dcdcbb11b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b183a95-bace-475f-9467-621d0c0037d8",
   "metadata": {},
   "source": [
    "## Fully connected Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3130808-5b4e-41de-b4ba-18e3651aeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29002fb-2e75-4920-9dc1-e4befbfd2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff33e6-ff4c-4e7c-8dc4-23f7ed5ef25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e621ec-593f-4c64-aae2-710d9ace7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop\n",
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    # print(y_predicted)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    # print(loss)\n",
    "    #backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "        y_test_preds = model(X_test)\n",
    "        test_loss = criterion(y_test_preds, y_test)\n",
    "        print('test loss: ' + str(test_loss))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')\n",
    "    # test_loss = criterion(y_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a725b-808c-469d-9ac8-c163bcfe1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d233d55-d133-47ba-a706-912dad1d2088",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc921e2-5d48-45fc-a977-c86e26abe322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfeabf3-e7d0-4419-a0fb-dcaedf88716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "input_size = 4\n",
    "num_classes = 1\n",
    "epochs = 1000\n",
    "# batch_size = 100\n",
    "learning_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c763649-aeb7-4d3e-9570-8fd35874cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16,out_channels=32,kernel_size=3, dilation =1) \n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(7936, 288) \n",
    "        self.fc2 = nn.Linear(288, 144) \n",
    "        self.fc3 = nn.Linear(144, num_classes) \n",
    "       \n",
    "    def forward(self, x):\n",
    "        bs, seq_len, num_channels = x.shape\n",
    "        x = x.view(bs, num_channels, seq_len)\n",
    "        out = self.pool(F.relu(self.conv1(x)))\n",
    "        out = self.pool(F.relu(self.conv2(out)))\n",
    "        out = out.view(bs, -1)\n",
    "        #print('lnear shape:', out.shape[1])\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        #out = F.relu(self.fc3(out))\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093d997-7cdc-48f6-a008-244f702a80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(input_size, num_classes).to(device)\n",
    "#model = Simple(1000, 4, 1).to(device)\n",
    "#criterion(y_train[0:8], yp)\n",
    "#yp.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f57a5-31d7-4435-98b1-af2013b278b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.Series(yp.squeeze().detach().cpu().numpy())\n",
    "s.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16981d-1038-45bf-aa87-d7c15ecab485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee3acb-0b37-476a-a354-9e7bf00e99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cafda1-28e8-4237-8ad2-d8ad1138d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop\n",
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "\n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "   \n",
    "    # print(y_predicted)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    # print(loss)\n",
    "    #backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "        y_test_preds = model(X_test)\n",
    "        test_loss = criterion(y_test_preds, y_test)\n",
    "        print('test loss: ' + str(test_loss))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')\n",
    "    # test_loss = criterion(y_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b95772-47e0-458c-895c-61084135f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(input_size, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da560820-9712-45ce-9f69-fc0b7717db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'lr': 1e-4,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 100,\n",
    "    'loss_fxn': 'b',\n",
    "    'opt': 'Adam'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9167b8c-522e-49ab-883e-9bc7dae0b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, train_loss = [], []\n",
    "CNNTrainer = TrainHelper(model = model,\n",
    "                      train_set = train_dataset,\n",
    "                      test_set = valid_dataset, opts = opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4109c-94df-4b97-9ad7-f33b555c4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNTrainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b1532-1c1c-4930-a053-6707117748cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35921f9f-3112-443e-9a42-0924b2058ea9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490da13-6148-4518-869b-e944cf8906d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=opts['batch_size']\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "first_data = train_loader.dataset[0]\n",
    "features,labels = first_data\n",
    "print(features,labels)\n",
    "\n",
    "\n",
    "# print(len(train_loader))\n",
    "# print(train_loader.dataset.x)\n",
    "# print(train_loader.dataset.y)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5c1f1-e24d-4c0a-99b8-3c3a5f284648",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data = test_loader.dataset[0]\n",
    "features,labels = first_data\n",
    "print(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888d8ec-3c0e-4418-a81a-99ef33bdc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features,labels = data\n",
    "print(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979c023-f21d-434a-a39b-a8770a6920e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da515c-fc66-4bcb-80f9-c019cc0bb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a79f8-65a7-4da3-a321-20030d9ea206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "input_size = 4\n",
    "num_classes = 1\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a888a-3abd-4ee2-8c7a-bfe314cea6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(input_size, num_classes).to(device = device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e389c9a-8f6c-4ad4-85c6-c11d621fced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05e7a7-a8e2-445e-9d75-90d42ede8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs,labels) in enumerate(train_loader):\n",
    "        # forward pass, backward pass, update\n",
    "        inputs = inputs.to(device=device, dtype=torch.float)\n",
    "        labels = labels.to(device=device, dtype=torch.float)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        #Backward and optimize\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #update weights\n",
    "        optimizer.step()\n",
    "        if (i+1) % 2000 ==0:\n",
    "            print(f'epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "print(\"Finish Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247376f-8480-438a-aa91-5d96bb835359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Finished Training')\n",
    "# PATH = './cnn.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec6657-af3d-4e3a-87f0-401adcc24a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device = device, dtype=torch.float)\n",
    "        labels = labels.to(device = device,  dtype=torch.float)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        print(len(labels))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            print(i)\n",
    "            label = labels[i]\n",
    "            print(len(label))\n",
    "            pred = predicted[i]\n",
    "            print(pred)\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3edf20-8c5d-43be-8f7a-7b7748d0b9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "730bb900-0662-4c3e-99c6-e07cc2ae74af",
   "metadata": {},
   "source": [
    "# Sherry's Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6ace9-0727-4079-8548-b5f20f3779d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_time = '0'\n",
    "mkpath = 'model/model%s'% save_model_time\n",
    "os.makedirs(mkpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cb4f2-50dd-4a8f-8c48-29dfde4681be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss.focal_loss import FocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7d2bb-aa60-4cd5-901a-4d184862071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainHelper():\n",
    "    '''\n",
    "    Helper class that makes it a bit easier and cleaner to define the training routine\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self,model,train_set,test_set,opts):\n",
    "        self.model = model  # neural net\n",
    "        # device agnostic code snippet\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.epochs = opts['epochs']\n",
    "        if opts['opt'] == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), opts['lr']) # optimizer method for gradient descent\n",
    "        else:\n",
    "            self.optimizer = torch.optim.SGD(model.parameters(), opts['lr'])\n",
    "        if opts['loss_fxn'] == 'c':\n",
    "            self.criterion = torch.nn.CrossEntropyLoss()                      # loss function\n",
    "        elif opts['loss_fxn'] == 'b':\n",
    "            self.criterion = torch.nn.BCELoss()                    # loss function used in papers\n",
    "        elif opts['loss_fxn'] == 'f':\n",
    "            self.criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                      batch_size=opts['batch_size'],\n",
    "                                                      shuffle=True)\n",
    "        self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                                      batch_size=opts['batch_size'],\n",
    "                                                      shuffle=True)\n",
    "    def train(self):\n",
    "        self.model.train() # put model in training mode\n",
    "        for epoch in range(self.epochs):\n",
    "            self.tr_loss = []\n",
    "            #105 = 10500/10(# batch size) \n",
    "            #print('what is this self.train loader ',len(self.train_loader))\n",
    "            for i, (data,labels) in tqdm_notebook(enumerate(self.train_loader),total = len(self.train_loader)):\n",
    "                label_list = []\n",
    "                \n",
    "                for i in range(len(labels)):\n",
    "                    #print('labels at i', labels[i])\n",
    "                    label_list.append(labels[i].to(self.device))\n",
    "                data = data.to(self.device)\n",
    "                self.optimizer.zero_grad()  \n",
    "                outputs = self.model(data)\n",
    "\n",
    "                b_list = []\n",
    "                for i in range(len(label_list)):\n",
    "                    #print('label list at i', label_list[i])\n",
    "                    b_list.append(label_list[i])\n",
    "                    #print('b list', b_list)\n",
    "                # if opts['loss_fxn'] != 'c':\n",
    "                #     for i in range(len(label_list)):\n",
    "                #         b_list[i] = torch.from_numpy(one_hot(labels[i])).to(self.device)\n",
    "\n",
    "                loss = 0  # define loss\n",
    "                #print('Test 1', b_list[1])\n",
    "                #print('Test 2', len(b_list))\n",
    "                for i in range(len(outputs)):\n",
    "                    #print(outputs[i], b_list[i])\n",
    "                    loss += self.criterion(outputs[i], b_list[i])\n",
    "                    #print('loss',loss)\n",
    "   \n",
    "                loss.backward()           \n",
    "                self.optimizer.step()                  \n",
    "                self.tr_loss.append(loss.item())       \n",
    "            if (epoch+1) % 5 == 0 or epoch == 0: # save the model every _ epoch\n",
    "                torch.save(self.model, 'model/model{save_model_time}/net_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
    "                torch.save(self.model.state_dict(), 'model/model{save_model_time}/net_params_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
    "          \n",
    "            self.test(epoch) # run through the validation set\n",
    "    def test(self,epoch):\n",
    "        self.model.eval()    # puts model in eval mode\n",
    "        self.test_loss = []\n",
    "        self.test_accuracy_L = [[] for _ in range(types)]\n",
    "\n",
    "        for i, (data, labels) in enumerate(self.valid_loader):\n",
    "            label_list = []\n",
    "            for i in range(len(labels)):\n",
    "                label_list.append(labels[i].to(self.device))\n",
    "            data = data.to(self.device)\n",
    "            # pass data through network\n",
    "            # turn off gradient calculation to speed up calcs and reduce memory\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(data)\n",
    "\n",
    "            # make our predictions and update our loss info\n",
    "            pred_list = []\n",
    "            print('length of outputs',len(outputs))\n",
    "            for i in range(len(outputs)):\n",
    "                print('outputs index i ',outputs[i])\n",
    "                _, predicted = torch.max(outputs[i].data, 1)\n",
    "                pred_list.append(predicted)\n",
    "\n",
    "            b_list = []\n",
    "            for i in range(len(label_list)):\n",
    "                b_list.append(label_list[i])\n",
    "            if opts['loss_fxn'] != 'c':\n",
    "                for i in range(len(label_list)):\n",
    "                    b_list[i] = torch.from_numpy(one_hot(labels[i])).to(self.device)\n",
    "\n",
    "            loss = 0  # define loss\n",
    "            for i in range(len(outputs)):\n",
    "                loss += self.criterion(outputs[i], b_list[i])\n",
    "\n",
    "            self.test_loss.append(loss.item())\n",
    "\n",
    "            for i in range(len(pred_list)):\n",
    "                self.test_accuracy_L[i].append((pred_list[i] == label_list[i]).sum().item() / pred_list[i].size(0))\n",
    "      \n",
    "        test_loss.append(np.mean(self.test_loss))\n",
    "        train_loss.append(np.mean(self.tr_loss))\n",
    "        av = [np.mean(self.test_accuracy_L[i]) for i in range(types)]\n",
    "        bestmodel(self.model,save_model_time,np.mean(self.test_loss)) # find best model\n",
    "        print('epoch: {}, train loss: {}, test loss: {}, test accuracy: {}'.format( \n",
    "            epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss), av))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "nn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
