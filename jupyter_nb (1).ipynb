{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eced0e5b-57b6-42b8-860c-52dc64e3a45e",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4582f7-d4d2-4e69-a06e-c9d965e77da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51042794-7f48-4ec6-8e93-4806599d5292",
   "metadata": {},
   "source": [
    "### Cuda debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d8dd9d-bc79-414f-a1fd-a8d53927bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f6ed1-29a4-4f25-bdbf-e604126e4978",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67fa35f-bd36-4312-bd34-b1ce96cf6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        \n",
    "    # def __getitem__(self, index):\n",
    "    #     return self.x[index], self.y[index]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53341565-bcae-4620-ab4f-a100a5ba8f44",
   "metadata": {},
   "source": [
    "# Parse fasta file and create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eba8ba0-e3ea-4ef1-8a55-c5cb21dcb404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: chr1:925238-926238\n",
      "Name: chr1:925238-926238\n",
      "Description: chr1:925238-926238\n",
      "Number of features: 0\n",
      "Seq('acagcagtatcctacctaagaagacttttgcccaaggtctttccaaacccaaga...cca')\n",
      "7291\n"
     ]
    }
   ],
   "source": [
    "# Load Fasta with biopython parser\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "# Create list for pos training data\n",
    "my_seqlist = []\n",
    "for seq_record in SeqIO.parse(open('/home/jovyan/data1/jjaureguy/nn_fasta/GSE168881/SRR13961069.fa.out'),'fasta'):\n",
    "    my_seqlist.append(seq_record)\n",
    "my_seqlist[0].seq\n",
    "\n",
    "print(my_seqlist[0])\n",
    "print(len(my_seqlist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed0cc74-e05a-4fa1-b8cb-a976b376bd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'A': 0, 'C': 1, 'T': 2, 'G': 3}, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bases = 'ACTG'\n",
    "base_map = dict(zip(bases, range(len(bases))))\n",
    "n_classes = len(base_map)\n",
    "base_map, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1a2b0e-d5f9-45c9-9c00-98c5cd83e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed98d8e9-3d5e-4ccd-9568-e1d5e5a45aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeafe5ca-e6fb-43d2-bb65-8a11632a39ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14566, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_seqlist = []\n",
    "neg_seqlist = []\n",
    "\n",
    "skipped = 0\n",
    "for i in my_seqlist:\n",
    "    fasta = i.seq\n",
    "    as_text = fasta.upper()\n",
    "    if 'N' in as_text:\n",
    "        skipped += 1\n",
    "    else:\n",
    "        seq = np.vectorize(base_map.get)(np.array(list(as_text)))\n",
    "        pos_seqlist.append(seq)\n",
    "        neg_seqlist.append(np.random.choice(seq, size=len(seq), replace=False))\n",
    "    \n",
    "seqlist = np.stack(pos_seqlist + neg_seqlist)\n",
    "#seqlist = list(filter(None, seqlist))  # This removes empty sequences.\n",
    "n_samples = len(seqlist)\n",
    "n_samples, skipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113555e6-7f19-482b-8719-b72c534726b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14566, 1000, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = one_hot(seqlist, n_classes).reshape(n_samples, -1, n_classes)\n",
    "training_data.shape\n",
    "#training_data.ndim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c22b4c-9e7b-48ae-842b-09db854eccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_log = training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b0588-d450-4eeb-a242-b032a4892152",
   "metadata": {},
   "source": [
    "# Create labels(response variable Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "229c1e98-33a4-45c3-ae72-18b3f0b6e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14566\n",
      "[1, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_var_pos = [1]*(n_samples//2)\n",
    "response_var_neg = [0]*(n_samples//2)\n",
    "\n",
    "total_response_var = response_var_pos + response_var_neg\n",
    "print(len(total_response_var))\n",
    "print(total_response_var[7282:7285])\n",
    "total_response_var = np.array(total_response_var)\n",
    "total_response_var.shape\n",
    "total_response_var.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806e385f-37d2-40ae-9384-8ec8c8a099aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "types = len(total_response_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a8872-8c72-40bd-ac38-d53ca3f10c5b",
   "metadata": {},
   "source": [
    "# Moving the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "214e63d5-a583-4210-95d6-93b6c62a13f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d60639-5404-4ddd-a81d-c3881c220f3c",
   "metadata": {},
   "source": [
    "# Splitting data set(training and test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64327697-ff1e-4918-a483-49e1fe3c698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11652, 1000, 4)\n",
      "(2914, 1000, 4)\n",
      "(11652,)\n",
      "(2914,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#print(x_log.shape)\n",
    "#print(total_response_var.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_log, total_response_var, train_size=0.8, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c92ca-2b35-4e8b-8619-0bdb584759ce",
   "metadata": {},
   "source": [
    "# Split training set into validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d03dd0-e147-4880-8137-c6b816b747ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712b45b6-b7d2-48d6-b7dd-52d6ad272b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10486, 1000, 4)\n",
      "(1166, 1000, 4)\n",
      "(10486,)\n",
      "(1166,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079dfce-f021-4d55-9eac-a3f274fb6b6b",
   "metadata": {},
   "source": [
    "# Converting dataset from np array to tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3859f394-7b64-477d-a7c4-e7d3ede20bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32)).to(device)\n",
    "X_train.requires_grad = True\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32)).to(device)\n",
    "X_test.requires_grad = True\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).to(device)\n",
    "y_train.requires_grad = True\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).to(device)\n",
    "y_test.requires_grad = True\n",
    "X_valid = torch.from_numpy(X_valid.astype(np.float32)).to(device)\n",
    "X_valid.requires_grad = True\n",
    "y_valid = torch.from_numpy(y_valid.astype(np.float32)).to(device)\n",
    "y_valid.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96883db5-9274-4042-b1d9-8aeb6394fd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.type()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "168c215e-16fe-416a-850f-54b35d480a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a81193f-05e3-49ba-8cef-6219752a7a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 0., 1.], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1806a3-6d62-441b-bdac-7949f7746f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Apply DataSet function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50f42c33-08de-4c95-8d30-738695f07d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "y_valid = y_valid.reshape(y_valid.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d73f6536-b363-479e-91a1-aaf5b1625677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2914, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 72%-20%-8% split\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "071bb1e4-ba7d-407c-ae23-bd670d223757",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_valid, y_valid)\n",
    "test_dataset = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17c6c54b-ba9d-4e21-97e1-279a20aad6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10486"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "418c7acf-4d40-4db8-be90-621f50738590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10486"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efd23064-e9b8-4eb9-85de-5148068a422a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_dataset.y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "801d0f80-589c-468b-afe8-2c511f2ea43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10486\n",
      "tensor([1.], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset.y[0])\n",
    "#print(train_dataset[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aafe29cb-ef63-4d62-9951-145cecb83533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.x.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dbc693b-3c27-4163-95b6-22335b87bca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         ...,\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         ...,\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.]],\n",
       "\n",
       "        [[0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9768658b-59e5-4b51-9c8a-f0a680764cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ab057-a770-472d-a6ef-7b9136b9732e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a29aa-5d9d-4351-9c4b-a09c6d3fab9e",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d9796-91dc-45f5-96d1-1428bc0fc99b",
   "metadata": {},
   "source": [
    "### Log regression var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb8a76-4221-4740-be5c-7b9ebd6a91bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#training_data.reshape(n_samples, -1).shape\n",
    "x_log = training_data.reshape(n_samples, -1)\n",
    "x_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e51e13-8b7b-49f4-b1c2-86b5dc5fe8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(x_log.shape)\n",
    "print(total_response_var.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_log, total_response_var, test_size=0.1, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ef776-b8d9-4b3d-bfdb-cb866c14e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846bb7b-c566-47ed-96d2-7f4b4d9b1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32)).to(device)\n",
    "X_train.requires_grad = True\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32)).to(device)\n",
    "X_test.requires_grad = True\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).to(device)\n",
    "y_train.requires_grad = True\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).to(device)\n",
    "y_test.requires_grad = True\n",
    "X_valid = torch.from_numpy(X_valid.astype(np.float32)).to(device)\n",
    "X_valid.requires_grad = True\n",
    "y_valid = torch.from_numpy(y_valid.astype(np.float32)).to(device)\n",
    "y_valid.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a82df-bd4b-4405-abf4-d6e6cf8e1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f4c3b-c5a7-4b1c-a877-99fc204c9cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c28e5-a0ab-4f47-a750-9c06f02b80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffb4a0-f5e3-42b0-b4e4-8a54dbf50728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce7109-fe60-45dd-a824-64f6a53a4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "epochs = 100\n",
    "input_dim = 4000  # Two inputs x1 and x2 \n",
    "output_dim = 1 # Single binary output \n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(input_dim,output_dim).to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b96c5-4704-4321-966c-ea1cd59e448b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7b55e-a437-49e0-86a8-1eddd5bee39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a6828-a5ca-47d7-ad2e-4866dca11f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.view(y_train.shape[0],1)\n",
    "# y_test = y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5f10d-146a-4941-9198-d14e7d09fe62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ea1db-c5df-4d7c-b4ff-5dcdcbb11b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b183a95-bace-475f-9467-621d0c0037d8",
   "metadata": {},
   "source": [
    "## Fully connected Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8ff54-ae89-490b-a386-0d9a7f93a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 4000\n",
    "hidden_size = 2000\n",
    "num_classes = 1\n",
    "epochs = 100\n",
    "# batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3130808-5b4e-41de-b4ba-18e3651aeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29002fb-2e75-4920-9dc1-e4befbfd2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff33e6-ff4c-4e7c-8dc4-23f7ed5ef25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6699e-cbe7-4972-8646-afe299243bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.dim\n",
    "#X_train.dim\n",
    "#y_test.dim\n",
    "X_test.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e621ec-593f-4c64-aae2-710d9ace7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop\n",
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    # print(y_predicted)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    # print(loss)\n",
    "    #backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "        y_test_preds = model(X_test)\n",
    "        test_loss = criterion(y_test_preds, y_test)\n",
    "        print('test loss: ' + str(test_loss))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')\n",
    "    # test_loss = criterion(y_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a725b-808c-469d-9ac8-c163bcfe1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d233d55-d133-47ba-a706-912dad1d2088",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205898f-2e97-4ff1-a209-1f5b87f82969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cc921e2-5d48-45fc-a977-c86e26abe322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bfeabf3-e7d0-4419-a0fb-dcaedf88716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "input_size = 4\n",
    "num_classes = 1\n",
    "epochs = 1000\n",
    "# batch_size = 100\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c763649-aeb7-4d3e-9570-8fd35874cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16,out_channels=32,kernel_size=3, dilation =1) \n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(7936, 288) \n",
    "        self.fc2 = nn.Linear(288, 144) \n",
    "        self.fc3 = nn.Linear(144, num_classes) \n",
    "       \n",
    "    def forward(self, x):\n",
    "        bs, seq_len, num_channels = x.shape\n",
    "        x = x.view(bs, num_channels, seq_len)\n",
    "        out = self.pool(F.relu(self.conv1(x)))\n",
    "        out = self.pool(F.relu(self.conv2(out)))\n",
    "        out = out.view(bs, -1)\n",
    "        #print('lnear shape:', out.shape[1])\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        #out = F.relu(self.fc3(out))\n",
    "        out = self.fc3(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1093d997-7cdc-48f6-a008-244f702a80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(input_size, num_classes).to(device)\n",
    "#model = Simple(1000, 4, 1).to(device)\n",
    "#criterion(y_train[0:8], yp)\n",
    "#yp.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "440f57a5-31d7-4435-98b1-af2013b278b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1340585/802943186.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yp' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "s = pd.Series(yp.squeeze().detach().cpu().numpy())\n",
    "s.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf16981d-1038-45bf-aa87-d7c15ecab485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29ee3acb-0b37-476a-a354-9e7bf00e99aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  1 07:15:47 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:65:00.0 Off |                    0 |\n",
      "|  0%   36C    P0    86W / 300W |   2449MiB / 22817MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1340585      C   ...da/envs/nn_env/bin/python     2449MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66cafda1-28e8-4237-8ad2-d8ad1138d6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10,loss = 0.6931\n",
      "test loss: tensor(0.6933, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 20,loss = 0.6922\n",
      "test loss: tensor(0.6927, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 30,loss = 0.6874\n",
      "test loss: tensor(0.6899, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 40,loss = 0.6643\n",
      "test loss: tensor(0.6771, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 50,loss = 0.6151\n",
      "test loss: tensor(0.6540, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 60,loss = 0.5893\n",
      "test loss: tensor(0.6387, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 70,loss = 0.5228\n",
      "test loss: tensor(0.6045, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 80,loss = 0.4747\n",
      "test loss: tensor(0.5912, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 90,loss = 0.4549\n",
      "test loss: tensor(0.5753, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 100,loss = 0.4526\n",
      "test loss: tensor(0.6002, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 110,loss = 0.3822\n",
      "test loss: tensor(0.5732, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 120,loss = 0.4005\n",
      "test loss: tensor(0.6127, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 130,loss = 0.3837\n",
      "test loss: tensor(0.5763, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 140,loss = 0.3382\n",
      "test loss: tensor(0.5507, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 150,loss = 0.3099\n",
      "test loss: tensor(0.5474, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 160,loss = 0.2945\n",
      "test loss: tensor(0.5476, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 170,loss = 0.2816\n",
      "test loss: tensor(0.5476, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 180,loss = 0.2851\n",
      "test loss: tensor(0.5483, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 190,loss = 0.2603\n",
      "test loss: tensor(0.5615, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 200,loss = 0.2551\n",
      "test loss: tensor(0.5517, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 210,loss = 0.2945\n",
      "test loss: tensor(0.5982, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 220,loss = 0.2586\n",
      "test loss: tensor(0.5806, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 230,loss = 0.2773\n",
      "test loss: tensor(0.6767, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 240,loss = 0.2216\n",
      "test loss: tensor(0.5694, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 250,loss = 0.2330\n",
      "test loss: tensor(0.5846, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 260,loss = 0.2121\n",
      "test loss: tensor(0.5752, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 270,loss = 0.1904\n",
      "test loss: tensor(0.5763, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 280,loss = 0.3379\n",
      "test loss: tensor(0.8650, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 290,loss = 0.2622\n",
      "test loss: tensor(0.6258, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 300,loss = 0.1998\n",
      "test loss: tensor(0.6115, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 310,loss = 0.1685\n",
      "test loss: tensor(0.5962, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 320,loss = 0.1647\n",
      "test loss: tensor(0.6035, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 330,loss = 0.1551\n",
      "test loss: tensor(0.6074, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 340,loss = 0.1591\n",
      "test loss: tensor(0.6369, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 350,loss = 0.2967\n",
      "test loss: tensor(0.8784, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 360,loss = 0.2069\n",
      "test loss: tensor(0.5840, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 370,loss = 0.1526\n",
      "test loss: tensor(0.6175, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 380,loss = 0.1430\n",
      "test loss: tensor(0.6170, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 390,loss = 0.1323\n",
      "test loss: tensor(0.6316, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 400,loss = 0.1257\n",
      "test loss: tensor(0.6462, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 410,loss = 0.1188\n",
      "test loss: tensor(0.6540, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 420,loss = 0.1127\n",
      "test loss: tensor(0.6663, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 430,loss = 0.1067\n",
      "test loss: tensor(0.6780, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 440,loss = 0.1115\n",
      "test loss: tensor(0.7191, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 450,loss = 1.4089\n",
      "test loss: tensor(0.8101, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 460,loss = 0.5785\n",
      "test loss: tensor(0.5334, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 470,loss = 0.4319\n",
      "test loss: tensor(0.5178, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 480,loss = 0.2485\n",
      "test loss: tensor(0.5374, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 490,loss = 0.1911\n",
      "test loss: tensor(0.5476, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 500,loss = 0.1760\n",
      "test loss: tensor(0.5471, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 510,loss = 0.1532\n",
      "test loss: tensor(0.5715, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 520,loss = 0.1352\n",
      "test loss: tensor(0.5987, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 530,loss = 0.1183\n",
      "test loss: tensor(0.6365, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 540,loss = 0.1038\n",
      "test loss: tensor(0.6805, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 550,loss = 0.0921\n",
      "test loss: tensor(0.7254, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 560,loss = 0.0816\n",
      "test loss: tensor(0.7705, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 570,loss = 0.0768\n",
      "test loss: tensor(0.8363, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 580,loss = 0.8954\n",
      "test loss: tensor(2.8639, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 590,loss = 0.6543\n",
      "test loss: tensor(0.5507, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 600,loss = 0.5275\n",
      "test loss: tensor(0.6177, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 610,loss = 0.4162\n",
      "test loss: tensor(0.5612, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 620,loss = 0.3769\n",
      "test loss: tensor(0.5342, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 630,loss = 0.3397\n",
      "test loss: tensor(0.5156, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 640,loss = 0.2985\n",
      "test loss: tensor(0.5055, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 650,loss = 0.2681\n",
      "test loss: tensor(0.5009, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 660,loss = 0.2425\n",
      "test loss: tensor(0.5008, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 670,loss = 0.2195\n",
      "test loss: tensor(0.5038, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 680,loss = 0.1991\n",
      "test loss: tensor(0.5098, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 690,loss = 0.1811\n",
      "test loss: tensor(0.5187, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 700,loss = 0.1652\n",
      "test loss: tensor(0.5298, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 710,loss = 0.1511\n",
      "test loss: tensor(0.5430, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 720,loss = 0.1386\n",
      "test loss: tensor(0.5579, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 730,loss = 0.1274\n",
      "test loss: tensor(0.5744, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 740,loss = 0.1172\n",
      "test loss: tensor(0.5921, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 750,loss = 0.1079\n",
      "test loss: tensor(0.6110, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 760,loss = 0.0993\n",
      "test loss: tensor(0.6310, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 770,loss = 0.0913\n",
      "test loss: tensor(0.6530, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 780,loss = 0.0831\n",
      "test loss: tensor(0.6784, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 790,loss = 0.0765\n",
      "test loss: tensor(0.6997, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 800,loss = 0.0706\n",
      "test loss: tensor(0.7201, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 810,loss = 0.0650\n",
      "test loss: tensor(0.7406, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 820,loss = 0.0597\n",
      "test loss: tensor(0.7613, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 830,loss = 0.0549\n",
      "test loss: tensor(0.7822, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 840,loss = 0.0503\n",
      "test loss: tensor(0.8033, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 850,loss = 0.0462\n",
      "test loss: tensor(0.8242, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 860,loss = 0.0424\n",
      "test loss: tensor(0.8445, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 870,loss = 0.0389\n",
      "test loss: tensor(0.8646, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 880,loss = 0.0358\n",
      "test loss: tensor(0.8842, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 890,loss = 0.0329\n",
      "test loss: tensor(0.9037, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 900,loss = 0.0303\n",
      "test loss: tensor(0.9230, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 910,loss = 0.0279\n",
      "test loss: tensor(0.9415, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 920,loss = 0.0258\n",
      "test loss: tensor(0.9596, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 930,loss = 0.0238\n",
      "test loss: tensor(0.9772, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 940,loss = 0.0220\n",
      "test loss: tensor(0.9955, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 950,loss = 0.0203\n",
      "test loss: tensor(1.0140, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 960,loss = 0.0187\n",
      "test loss: tensor(1.0322, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 970,loss = 0.0173\n",
      "test loss: tensor(1.0508, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 980,loss = 0.0160\n",
      "test loss: tensor(1.0684, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 990,loss = 0.0149\n",
      "test loss: tensor(1.0852, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "epoch: 1000,loss = 0.0139\n",
      "test loss: tensor(1.1009, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "training accuracy =  0.0003\n",
      "accuracy =  0.0443\n"
     ]
    }
   ],
   "source": [
    "#Loop\n",
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "\n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "   \n",
    "    # print(y_predicted)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    # print(loss)\n",
    "    #backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "        y_test_preds = model(X_test)\n",
    "        test_loss = criterion(y_test_preds, y_test)\n",
    "        print('test loss: ' + str(test_loss))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')\n",
    "    # test_loss = criterion(y_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b95772-47e0-458c-895c-61084135f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(input_size, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da560820-9712-45ce-9f69-fc0b7717db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'lr': 1e-4,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 100,\n",
    "    'loss_fxn': 'b',\n",
    "    'opt': 'Adam'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9167b8c-522e-49ab-883e-9bc7dae0b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, train_loss = [], []\n",
    "CNNTrainer = TrainHelper(model = model,\n",
    "                      train_set = train_dataset,\n",
    "                      test_set = valid_dataset, opts = opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4109c-94df-4b97-9ad7-f33b555c4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNTrainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b1532-1c1c-4930-a053-6707117748cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35921f9f-3112-443e-9a42-0924b2058ea9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490da13-6148-4518-869b-e944cf8906d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=opts['batch_size']\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "first_data = train_loader.dataset[0]\n",
    "features,labels = first_data\n",
    "print(features,labels)\n",
    "\n",
    "\n",
    "# print(len(train_loader))\n",
    "# print(train_loader.dataset.x)\n",
    "# print(train_loader.dataset.y)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5c1f1-e24d-4c0a-99b8-3c3a5f284648",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data = test_loader.dataset[0]\n",
    "features,labels = first_data\n",
    "print(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888d8ec-3c0e-4418-a81a-99ef33bdc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features,labels = data\n",
    "print(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979c023-f21d-434a-a39b-a8770a6920e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da515c-fc66-4bcb-80f9-c019cc0bb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a79f8-65a7-4da3-a321-20030d9ea206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "input_size = 4\n",
    "num_classes = 1\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a888a-3abd-4ee2-8c7a-bfe314cea6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(input_size, num_classes).to(device = device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e389c9a-8f6c-4ad4-85c6-c11d621fced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a312f2f-2391-40ac-9dbc-8611ba9e2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05e7a7-a8e2-445e-9d75-90d42ede8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "n_total_steps = len(train_loader)\n",
    "correct_pred = 0\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, (inputs,labels) in enumerate(train_loader):\n",
    "        # forward pass, backward pass, update\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        inputs = inputs.to(device=device, dtype=torch.float)\n",
    "        labels = labels.to(device=device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(inputs)\n",
    "        loss = criterion(output,labels)\n",
    "        acc = binary_acc(output,labels) \n",
    "        #Backward and optimize\n",
    "        y_pred_tag = torch.round(torch.sigmoid(output))\n",
    "\n",
    "        # correct_results_sum = (y_pred_tag == labels).sum().float()\n",
    "        # acc = correct_results_sum/output[0]\n",
    "        # acc = torch.round(acc * 100)\n",
    "        \n",
    "        loss.backward()\n",
    "        #update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # epoch_loss += loss.item()\n",
    "        # epoch_acc += acc.item()\n",
    "    # print(f'Epoch {epoch+1}/{num_epochs}: | Loss: {loss.item():.4f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "        if (i+1) % 100 ==0:\n",
    "            print(f'epoch [{epoch+1}/{num_epochs}],Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f} Acc: {acc.item/n_total_steps}')\n",
    "    \n",
    "print(\"Finish Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3edf20-8c5d-43be-8f7a-7b7748d0b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')\n",
    "    # test_loss = criterion(y_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499da7a-8d23-4631-a5d9-acecdc875463",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247376f-8480-438a-aa91-5d96bb835359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Finished Training')\n",
    "# PATH = './cnn.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec6657-af3d-4e3a-87f0-401adcc24a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device=device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bb900-0662-4c3e-99c6-e07cc2ae74af",
   "metadata": {},
   "source": [
    "# Sherry's Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6ace9-0727-4079-8548-b5f20f3779d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_time = '0'\n",
    "mkpath = 'model/model%s'% save_model_time\n",
    "os.makedirs(mkpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cb4f2-50dd-4a8f-8c48-29dfde4681be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss.focal_loss import FocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7d2bb-aa60-4cd5-901a-4d184862071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainHelper():\n",
    "    '''\n",
    "    Helper class that makes it a bit easier and cleaner to define the training routine\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self,model,train_set,test_set,opts):\n",
    "        self.model = model  # neural net\n",
    "        # device agnostic code snippet\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.epochs = opts['epochs']\n",
    "        if opts['opt'] == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), opts['lr']) # optimizer method for gradient descent\n",
    "        else:\n",
    "            self.optimizer = torch.optim.SGD(model.parameters(), opts['lr'])\n",
    "        if opts['loss_fxn'] == 'c':\n",
    "            self.criterion = torch.nn.CrossEntropyLoss()                      # loss function\n",
    "        elif opts['loss_fxn'] == 'b':\n",
    "            self.criterion = torch.nn.BCELoss()                    # loss function used in papers\n",
    "        elif opts['loss_fxn'] == 'f':\n",
    "            self.criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                      batch_size=opts['batch_size'],\n",
    "                                                      shuffle=True)\n",
    "        self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                                      batch_size=opts['batch_size'],\n",
    "                                                      shuffle=True)\n",
    "    def train(self):\n",
    "        self.model.train() # put model in training mode\n",
    "        for epoch in range(self.epochs):\n",
    "            self.tr_loss = []\n",
    "            #105 = 10500/10(# batch size) \n",
    "            #print('what is this self.train loader ',len(self.train_loader))\n",
    "            for i, (data,labels) in tqdm_notebook(enumerate(self.train_loader),total = len(self.train_loader)):\n",
    "                label_list = []\n",
    "                \n",
    "                for i in range(len(labels)):\n",
    "                    #print('labels at i', labels[i])\n",
    "                    label_list.append(labels[i].to(self.device))\n",
    "                data = data.to(self.device)\n",
    "                self.optimizer.zero_grad()  \n",
    "                outputs = self.model(data)\n",
    "\n",
    "                b_list = []\n",
    "                for i in range(len(label_list)):\n",
    "                    #print('label list at i', label_list[i])\n",
    "                    b_list.append(label_list[i])\n",
    "                    #print('b list', b_list)\n",
    "                # if opts['loss_fxn'] != 'c':\n",
    "                #     for i in range(len(label_list)):\n",
    "                #         b_list[i] = torch.from_numpy(one_hot(labels[i])).to(self.device)\n",
    "\n",
    "                loss = 0  # define loss\n",
    "                #print('Test 1', b_list[1])\n",
    "                #print('Test 2', len(b_list))\n",
    "                for i in range(len(outputs)):\n",
    "                    #print(outputs[i], b_list[i])\n",
    "                    loss += self.criterion(outputs[i], b_list[i])\n",
    "                    #print('loss',loss)\n",
    "   \n",
    "                loss.backward()           \n",
    "                self.optimizer.step()                  \n",
    "                self.tr_loss.append(loss.item())       \n",
    "            if (epoch+1) % 5 == 0 or epoch == 0: # save the model every _ epoch\n",
    "                torch.save(self.model, 'model/model{save_model_time}/net_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
    "                torch.save(self.model.state_dict(), 'model/model{save_model_time}/net_params_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
    "          \n",
    "            self.test(epoch) # run through the validation set\n",
    "    def test(self,epoch):\n",
    "        self.model.eval()    # puts model in eval mode\n",
    "        self.test_loss = []\n",
    "        self.test_accuracy_L = [[] for _ in range(types)]\n",
    "\n",
    "        for i, (data, labels) in enumerate(self.valid_loader):\n",
    "            label_list = []\n",
    "            for i in range(len(labels)):\n",
    "                label_list.append(labels[i].to(self.device))\n",
    "            data = data.to(self.device)\n",
    "            # pass data through network\n",
    "            # turn off gradient calculation to speed up calcs and reduce memory\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(data)\n",
    "\n",
    "            # make our predictions and update our loss info\n",
    "            pred_list = []\n",
    "            print('length of outputs',len(outputs))\n",
    "            for i in range(len(outputs)):\n",
    "                print('outputs index i ',outputs[i])\n",
    "                _, predicted = torch.max(outputs[i].data, 1)\n",
    "                pred_list.append(predicted)\n",
    "\n",
    "            b_list = []\n",
    "            for i in range(len(label_list)):\n",
    "                b_list.append(label_list[i])\n",
    "            if opts['loss_fxn'] != 'c':\n",
    "                for i in range(len(label_list)):\n",
    "                    b_list[i] = torch.from_numpy(one_hot(labels[i])).to(self.device)\n",
    "\n",
    "            loss = 0  # define loss\n",
    "            for i in range(len(outputs)):\n",
    "                loss += self.criterion(outputs[i], b_list[i])\n",
    "\n",
    "            self.test_loss.append(loss.item())\n",
    "\n",
    "            for i in range(len(pred_list)):\n",
    "                self.test_accuracy_L[i].append((pred_list[i] == label_list[i]).sum().item() / pred_list[i].size(0))\n",
    "      \n",
    "        test_loss.append(np.mean(self.test_loss))\n",
    "        train_loss.append(np.mean(self.tr_loss))\n",
    "        av = [np.mean(self.test_accuracy_L[i]) for i in range(types)]\n",
    "        bestmodel(self.model,save_model_time,np.mean(self.test_loss)) # find best model\n",
    "        print('epoch: {}, train loss: {}, test loss: {}, test accuracy: {}'.format( \n",
    "            epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss), av))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "nn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
