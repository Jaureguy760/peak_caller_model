{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1331f97-618c-4acc-ac05-cf1d8c7078af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eb52f-cb3a-498c-b131-0594aa5ca66b",
   "metadata": {},
   "source": [
    "# Parse fasta file and create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60fc4a2-6e4b-4a26-8c81-435fa167eff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/iblm/netapp/data1/jjaureguy/HiChip_project/HiChip_peak_results/GSE168881/SRR13961069.fa.out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3686/185792398.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create list for pos training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_seqlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mseq_record\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/iblm/netapp/data1/jjaureguy/HiChip_project/HiChip_peak_results/GSE168881/SRR13961069.fa.out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fasta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmy_seqlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmy_seqlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/iblm/netapp/data1/jjaureguy/HiChip_project/HiChip_peak_results/GSE168881/SRR13961069.fa.out'"
     ]
    }
   ],
   "source": [
    "# Load Fasta with biopython parser\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "# Create list for pos training data\n",
    "my_seqlist = []\n",
    "for seq_record in SeqIO.parse(open('/iblm/netapp/data1/jjaureguy/HiChip_project/HiChip_peak_results/GSE168881/SRR13961069.fa.out'),'fasta'):\n",
    "    my_seqlist.append(seq_record)\n",
    "my_seqlist[0].seq\n",
    "\n",
    "print(my_seqlist[0])\n",
    "print(len(my_seqlist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1280f902-392e-4f6f-ad94-7c294a75b96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'A': 0, 'C': 1, 'T': 2, 'G': 3}, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bases = 'ACTG'\n",
    "base_map = dict(zip(bases, range(len(bases))))\n",
    "n_classes = len(base_map)\n",
    "base_map, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b0a388-2ce8-4382-a92f-18ebc8ad97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f6e632-ac4f-44b2-9cdd-316914c43719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3311bd-dbe3-494d-a36e-8254e7bef8af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3686/3633430995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mneg_seqlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mseqlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_seqlist\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_seqlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#seqlist = list(filter(None, seqlist))  # This removes empty sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nn_env/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "pos_seqlist = []\n",
    "neg_seqlist = []\n",
    "\n",
    "skipped = 0\n",
    "for i in my_seqlist:\n",
    "    fasta = i.seq\n",
    "    as_text = fasta.upper()\n",
    "    if 'N' in as_text:\n",
    "        skipped += 1\n",
    "    else:\n",
    "        seq = np.vectorize(base_map.get)(np.array(list(as_text)))\n",
    "        pos_seqlist.append(seq)\n",
    "        neg_seqlist.append(np.random.choice(seq, size=len(seq), replace=False))\n",
    "    \n",
    "seqlist = np.stack(pos_seqlist + neg_seqlist)\n",
    "#seqlist = list(filter(None, seqlist))  # This removes empty sequences.\n",
    "n_samples = len(seqlist)\n",
    "n_samples, skipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c94a2c-38d5-442e-8161-fa018063d777",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seqlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3686/1855380009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seqlist' is not defined"
     ]
    }
   ],
   "source": [
    "training_data = one_hot(seqlist, n_classes).reshape(n_samples, -1, n_classes)\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef26f2-5d86-4e5f-92e1-419515047f76",
   "metadata": {},
   "source": [
    "# Log regression var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe19e59-80dc-42b0-a78b-64e39f4d803d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3686/1928134025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "training_data.reshape(n_samples, -1).shape\n",
    "x_log = training_data.reshape(n_samples, -1)\n",
    "x_log.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb3e56-72cb-4965-b85e-e149c082e8e2",
   "metadata": {},
   "source": [
    "# Create labels(response variable Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d64163-5bb0-44c9-996a-7a30d0909820",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_var_pos = [1]*(n_samples//2)\n",
    "response_var_neg = [0]*(n_samples//2)\n",
    "\n",
    "total_response_var = response_var_pos + response_var_neg\n",
    "print(len(total_response_var))\n",
    "print(total_response_var[7282:7285])\n",
    "total_response_var = np.array(total_response_var)\n",
    "total_response_var.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce51bb9-565a-40c2-afb7-57d8560e085e",
   "metadata": {},
   "source": [
    "# One Hot Encode Class(Extra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87acfe20-9e99-4f3c-a778-b6acc6c04648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hot_dna:\n",
    "    def __init__(self,fasta):\n",
    "        #check for and grab sequence name\n",
    "        name = 'unknown_sequence'\n",
    "        sequence = fasta\n",
    "        #get sequence into an array\n",
    "        seq_array = array(list(sequence))\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
    "    \n",
    "        #one hot the sequence\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        #reshape because that's what OneHotEncoder likes\n",
    "        integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
    "        onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
    "\n",
    "        #add the attributes to self \n",
    "        self.name = name\n",
    "        self.sequence = fasta\n",
    "        self.integer = integer_encoded_seq\n",
    "        self.onehot = onehot_encoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93402199-6960-4ff9-ae10-585b005dc119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fea9102-ceed-4be9-83a1-2a67dab114f1",
   "metadata": {},
   "source": [
    "# One hot encode training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e379fe-66c0-4940-81f3-fbc1a66715c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode training data \n",
    "one_hot_enc_train = []\n",
    "for i in seqlist:\n",
    "    hot_enc = hot_dna(i)\n",
    "    #print(hot_enc.integer)\n",
    "    one_hot_enc_train.append(hot_enc.onehot)\n",
    "#one_hot_enc[0]\n",
    "\n",
    "\n",
    "print(len(one_hot_enc_train))\n",
    "print(one_hot_enc_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35900bab-74fd-4ba2-9679-d71b1f8a5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = set()\n",
    "for seq in one_hot_enc_train:\n",
    "    shapes.add(seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914c4d0-ae43-42ae-bd62-f65f631c4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42254c8a-4236-47bd-9cba-80beaf53da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_hot_enc_train = np.stack(one_hot_enc_train)\n",
    "#one_hot_enc_train.toarray()\n",
    "print(len(one_hot_enc_train))\n",
    "\n",
    "print(one_hot_enc_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b5fb2-4334-4703-bda3-710f9d05ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2993f7c7-30ae-4448-b66f-d0817b871557",
   "metadata": {},
   "source": [
    "# Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5aafb-1e01-4f81-b1de-7cb3174f6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# The LabelEncoder encodes a sequence of bases as a sequence of integers.\n",
    "integer_encoder = LabelEncoder()  \n",
    "# The OneHotEncoder converts an array of integers to a sparse matrix where \n",
    "# each row corresponds to one possible value of each feature.\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')   \n",
    "# Positive training list\n",
    "input_features = []\n",
    "\n",
    "\n",
    "for sequence in total_train_data:\n",
    "    #print(sequence)\n",
    "    #fasta = sequence.seq\n",
    "    integer_encoded = integer_encoder.fit_transform(sequence)\n",
    "    #print(integer_encoded)\n",
    "    ##########RESHAPE issue##########(14,000,-1,1)\n",
    "    integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "    input_features.append(one_hot_encoded.toarray())\n",
    "\n",
    "np.set_printoptions(threshold=40)\n",
    "input_features = np.stack(input_features)\n",
    "\n",
    "#input_features = np.array(input_features)\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n',total_train_data[0][:10],'...',total_train_data[0][-10:])\n",
    "print('One hot encoding of Sequence #1:\\n',input_features[0].T)\n",
    "\n",
    "\n",
    "print(input_features.ndim)\n",
    "print(input_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078365d-78d2-4baf-9a3a-c6e6bdd8bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dimensions of arrays\n",
    "#one_hot_enc_train.ndim\n",
    "\n",
    "print(one_hot_enc_train[1])\n",
    "#Check input_features\n",
    "one_hot_enc_train = np.hstack(one_hot_enc_train)\n",
    "\n",
    "one_hot_enc_train = np.array(one_hot_enc_train)\n",
    "print('training data dimension:',one_hot_enc_train.ndim)\n",
    "print(('training data shape: ',one_hot_enc_train.shape))\n",
    "\n",
    "print(\"X var dimension:\",input_features.ndim)\n",
    "print(\"X var dimension:\",input_features.shape)\n",
    "\n",
    "#Check input_labels(response variables)\n",
    "print('Y var dimension: ',input_labels.ndim)\n",
    "print(('Y var shape:',input_labels[0].shape))\n",
    "#print(len(input_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ce3f0-893e-467a-b16b-c9189d3dfb10",
   "metadata": {},
   "source": [
    "# Additional training exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676c254-63e3-4e5d-868e-02058cd112cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "#random.shuffle(one_hot_enc)\n",
    "#prediction = list(my_seqlist.reverse)\n",
    "prediction_list = list(reversed(my_seqlist))\n",
    "print(len(prediction_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762c713-019b-4400-93b1-319fd39f7074",
   "metadata": {},
   "source": [
    "# Test one hot enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27581c6-185b-4d34-9dd6-f5fa4fdaddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc = []\n",
    "for i in my_seqlist:\n",
    "    fasta = i.seq\n",
    "    #print(fasta[:10])\n",
    "    hot_enc = hot_dna(fasta)\n",
    "    #print(hot_enc.integer)\n",
    "    one_hot_enc.append(hot_enc.onehot)\n",
    "#one_hot_enc[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7173b-25d5-4a29-bc89-4006c3e4b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(one_hot_enc))\n",
    "print(one_hot_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92eae8d-bfa3-4f0b-aa35-33c04372512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train[0].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed1199-fd45-403c-bed3-6f810a4b51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc_neg = []\n",
    "for i in neg_train:\n",
    "    fasta = i.seq\n",
    "    hot_enc = hot_dna(fasta)\n",
    "    #print(hot_enc.integer)\n",
    "    one_hot_enc_neg.append(hot_enc.onehot)\n",
    "#one_hot_enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ca3e7-c760-404b-8c28-6997c4c8f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a909eab-303d-427e-a300-e3d790ee7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(one_hot_enc_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b99d08-2694-4beb-be11-260f9c813d7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# One hot encode training data(alternative approach from tutorial) no producing correct dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4df83-132b-4d48-a1b2-5718c0f3a799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# The LabelEncoder encodes a sequence of bases as a sequence of integers.\n",
    "integer_encoder = LabelEncoder()  \n",
    "# The OneHotEncoder converts an array of integers to a sparse matrix where \n",
    "# each row corresponds to one possible value of each feature.\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')   \n",
    "# Positive training list\n",
    "input_features = []\n",
    "\n",
    "\n",
    "for sequence in total_train_data:\n",
    "    fasta = sequence.seq\n",
    "    integer_encoded = integer_encoder.fit_transform(list(fasta))\n",
    "    #print(integer_encoded)\n",
    "    integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "    input_features.append(one_hot_encoded.toarray())\n",
    "\n",
    "np.set_printoptions(threshold=40)\n",
    "input_features = np.hstack(input_features)\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n',total_tain_data[0][:10].seq,'...',total_tain_data[0][-10:].seq)\n",
    "print('One hot encoding of Sequence #1:\\n',input_features[0].T)\n",
    "\n",
    "\n",
    "print(len(input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1b04b-3740-4998-b720-8aa86c930dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# The LabelEncoder encodes a sequence of bases as a sequence of integers.\n",
    "integer_encoder = LabelEncoder()  \n",
    "# The OneHotEncoder converts an array of integers to a sparse matrix where \n",
    "# each row corresponds to one possible value of each feature.\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')   \n",
    "# Positive training list\n",
    "input_features = []\n",
    "# Negative training list\n",
    "input_features_neg = []\n",
    "# Prediction reversed one hot encode\n",
    "input_features_prediction_list = []\n",
    "#Postive training list one hot encode\n",
    "\n",
    "for sequence in my_seqlist:\n",
    "    fasta = sequence.seq\n",
    "    integer_encoded = integer_encoder.fit_transform((fasta))\n",
    "    #print(integer_encoded)\n",
    "    integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "    input_features.append(one_hot_encoded.toarray())\n",
    "\n",
    "np.set_printoptions(threshold=40)\n",
    "input_features = np.hstack(input_features)\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n',my_seqlist[0][:10].seq,'...',my_seqlist[0][-10:].seq)\n",
    "print('One hot encoding of Sequence #1:\\n',input_features[0].T)\n",
    "\n",
    "#Negative training list one hot encode\n",
    "\n",
    "for sequence in neg_train:\n",
    "    fasta = sequence.seq\n",
    "    integer_encoded = integer_encoder.fit_transform((fasta))\n",
    "    integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "    input_features_neg.append(one_hot_encoded.toarray())\n",
    "\n",
    "np.set_printoptions(threshold=40)\n",
    "input_features_neg = np.hstack(input_features_neg)\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n',neg_train[0][:10].seq,'...',neg_train[0][-10:].seq)\n",
    "print('One hot encoding of Sequence #1:\\n',input_features_neg[0].T)\n",
    "\n",
    "\n",
    "# #Prediction reversed one hot encode\n",
    "# for sequence in prediction_list:\n",
    "#     fasta = sequence.seq\n",
    "#     integer_encoded = integer_encoder.fit_transform((fasta))\n",
    "#     integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "#     one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "#     input_features_prediction_list.append(one_hot_encoded.toarray())\n",
    "\n",
    "# np.set_printoptions(threshold=40)\n",
    "# input_features_prediction_list = np.hstack(input_features_prediction_list)\n",
    "# print(\"Example sequence\\n-----------------------\")\n",
    "# print('DNA Sequence #1:\\n',prediction_list[0][:10].seq,'...',prediction_list[0][-10:].seq)\n",
    "# print('One hot encoding of Sequence #1:\\n',input_features_prediction_list[0].T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c63ab-eaa9-4703-a454-0bc884b306a8",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4472b1-1910-4839-be2d-3d0146f46300",
   "metadata": {},
   "source": [
    "#Split data into a train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2885967-7fbc-48cc-99fb-c857633ef2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(x_log.shape)\n",
    "print(total_response_var.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_log, total_response_var, test_size=0.1, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c653e4c-2e53-4f8e-a4ac-7afd393de435",
   "metadata": {},
   "source": [
    "#Build the pytorch logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaeb1d8-d462-4cc1-a16d-df3e22e7d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04813b49-86b1-45a0-9427-c74ff472ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 10000\n",
    "input_dim = 4000  # Two inputs x1 and x2 \n",
    "output_dim = 1 # Single binary output \n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(input_dim,output_dim)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ccfdda-e115-439e-a1a5-210fd41e9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_train.requires_grad = True\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "X_test.requires_grad = True\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_train.requires_grad = True\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "y_test.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d907eb-8498-4796-b374-c793bff04c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e8ef1-bbb6-45c6-839b-8f78b68563e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43887165-b28e-4ef7-b177-6d95d0342b11",
   "metadata": {},
   "source": [
    "# Linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e9769-25c7-4e1e-bd90-ab2eff5ec1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5aed1-9622-4c3f-9599-9366d38a82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 10000\n",
    "input_dim = 4000  # Two inputs x1 and x2 \n",
    "output_dim = 1 # Single binary output \n",
    "learning_rate = 0.01\n",
    "\n",
    "#model\n",
    "model = nn.Linear(input_dim,output_dim)\n",
    "#Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cea91-6071-42c9-a4a6-a939a208cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop\n",
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe91d83-868d-4a2c-b842-4c4f1dccfa65",
   "metadata": {},
   "source": [
    "# CNN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64cbfd-7abf-417a-b381-44884f7d62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b9a53-83c4-44ad-abe7-431537a54d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790698c3-1e5c-4ba7-a0ee-f3f4b6092682",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d973602-d0b7-4f71-a07f-a4187c800bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_log, total_response_var, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f77e40-5ce2-4ea4-8968-68f61da36cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_train.requires_grad = True\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "X_test.requires_grad = True\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_train.requires_grad = True\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "y_test.requires_grad = True\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c9f6b-e3a9-4e89-8ab8-1482ccfa04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50bf72-9235-47d8-9e18-cbc8870ede3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "input_size = 1000*4\n",
    "hidden_size = 2000\n",
    "classes = 1\n",
    "epochs = 140\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1e61e-0d43-4096-a5d5-9c537b5b3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the network\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class test_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test_net,self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(i)\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,1000)\n",
    "        self.fc3 = nn.Linear(1000,classes)\n",
    "    def forward(self,X):\n",
    "        # out = out.flatten()\n",
    "        out = self.fc1(X)\n",
    "        out = torch.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.tanh(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221523c4-0af1-4481-954a-7582d7a06799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model\n",
    "model = test_net()\n",
    "#Loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77793a-91bd-4a6f-a691-adee778199d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop\n",
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    # print(y_predicted)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    # print(loss)\n",
    "    #backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "        y_test_preds = model(X_test)\n",
    "        test_loss = criterion(y_test_preds, y_test)\n",
    "        print('test loss: ' + str(test_loss))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')\n",
    "    # test_loss = criterion(y_predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06047195-e3f8-4052-9993-d3a6afcb3a89",
   "metadata": {},
   "source": [
    "# Fully connected Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f375cb2d-ae7b-4b02-8a80-a5fd651ce10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 4000\n",
    "hidden_size = 2000\n",
    "num_classes = 1\n",
    "epochs = 1000\n",
    "# batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40338c6-a6d8-457a-92a2-fc8d87f926a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8adbe3-0857-4a7d-ae36-c5866cecffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896bb22-fcf2-4cd5-ae5f-2b9f437a4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop\n",
    "for epoch in range(epochs):#\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.data)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    # print(y_predicted)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    # print(loss)\n",
    "    #backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    #zero out the gradients since backward function always adds up all gradients(empty before next iteration)\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print(f'epoch: {epoch+1},loss = {loss.item():.4f}')\n",
    "        y_test_preds = model(X_test)\n",
    "        test_loss = criterion(y_test_preds, y_test)\n",
    "        print('test loss: ' + str(test_loss))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    y_train_predicted = model(X_train).round()\n",
    "    acc = y_train_predicted.eq(y_train).sum() / float(y_train.shape[0])\n",
    "    print(f'training accuracy = {acc: .4f}')\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')\n",
    "    # test_loss = criterion(y_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec4613-fe8f-4ff9-998a-39336095303d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4841a-27a8-4c50-be50-1f2e1f2ac07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "nn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
